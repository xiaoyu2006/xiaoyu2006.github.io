---
title: "LLM will NEVER be AGI: The Proof"
date: 2024-08-20 21:00:00 +0800
tags: [programming, math]
math: true
---

The proof is trivial with a little help of a necessary condition of complexity theories. All LLM runs under the complexity of $O(n)$, where $n$ is the length of the output.

Suppose LLM is AGI, then it is able to solve any problem that a human can solve. Consider the following problem:

> Given a string $s$ of length $n$, determine whether $s$ is a palindrome. Answer "Y" if it is and "N" if it isn't.

Apprently, a human can solve this problem, and the problem must be solved in at least $O(n)$ time.

Since the output of this problem is a constant length, LLM must solve this problem in $O(1)$ time, which is a contradiction. Thus LLM cannot solve a problem that a human can solve. Therefore LLM is not AGI. $\blacksquare$
